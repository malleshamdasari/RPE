\section{Related Work}
There is an extensive prior work on video QoE modeling. The vast majority of works \cite{aggarwal2014prometheus,balachandran2013developing,fiedler2010generic} introduce machine learning methods to map QoS to QoE, and assume availability of the QoE ground-truth. Features for QoE estimation can be collected from various vantage points, such as on the application server~\cite{balachandran2013developing}, on the end-device (application-client statistics or packet capturing)~\cite{zhang2012profiling,yu2014can,seufert2015survey,aggarwal2014prometheus,balachandran2012quest,chen2014qoe}, or the access network (e.g., \wifi AP or LTE base station)~\cite{chakraborty2016exbox,jana2016qoe,chen2006quantifying}. Compared to the above works, we focus on annotating QoE ground-truth, hence easing the extension of QoS to QoE mappings to all video-telephony applications. Our work significantly advances state-of-the-art QoE labeling at the training phase of QoS to QoE, by introducing accurate metrics that capture spatial and temporal video quality.

\noindent
\textbf{Spatial quality assessment:}  Prior works have proposed multiple metrics to capture video blurriness (spatial artefacts). Jana et al.~\cite{jana2016qoe} introduce metrics for freezes, blocking and blur over Skype and Vtok. However, as we show in Section~\ref{MOTIVATION}, this blur metric is highly content dependent. We have found similar results for major prior works~\cite{golestaneh2014no, mittal2012no,tong2004blur,marziliano2002no} on no-reference blur detection. In Section V, we present more than 1 MOS lower estimation error for our metrics compared to prior work over 20 diverse videos. We have not observed blocking artefacts in any of Skype, FaceTime or Hangouts applications.

\noindent
\textbf{Temporal quality assessment:} Several works \cite{wolf2009no, borer2010model, usman2017no, pastrana2006automatic} have investigated the effect of video freezes on user QoE. Wolf and Pinson~\cite{wolf2009no} propose to use the motion energy temporal history of videos, along with framerate of video. This requires additional information from original video hence, making it a reduced reference metric. Similarly, the temporal metric of Jana et al.~\cite{jana2016qoe} is a reduced-reference metric. The temporal metric by Pastrana-Vidal and Gicquel~\cite{pastrana2006automatic} is sensitive to resolution and content of the video. Different from all of these works, we present three content-independent, no-reference temporal metrics to assess temporal video artefacts.
