\section{QoE Labeling}

Over the past decade, mobile video traffic has increased dramatically (from 50\% in year 2011 to 60\% in 2016 and is predicted to reach 78\% by 2021)~\cite{forecast2016cisco}. This is due to the proliferation of mobile video applications (such as Skype, FaceTime, Hangouts, YouTube, and Netflix etc). These applications can be categorized into video telephony (Skype, Hangouts, FaceTime), streaming (YouTube, Netflix), and upcoming virtual reality and augmented reality streaming (SPT~\cite{spt}, theBlu~\cite{theblu}). 
Users demand high Quality of Experience (QoE) while using these applications on wireless networks, such as WiFi and LTE. This poses a unique challenge for network administrators in enterprise environments, such as offices, university campuses and retail stores.

Guaranteeing best possible QoE is non-trivial because of several factors in video delivery path (such as network conditions at the client side and server side, client device, video compression standard, video application). While application content providers focus on improving the server-side network performance, video compression and application logic, enterprise network administrators seek to ensure that network resources are well provisioned and managed to provide good experience to multiple users of diverse applications. In this pursuit, network administrators can only rely on passive in-network measurements to {\em estimate} the exact user experience on the end-device. In this context, several prior works have developed a mapping between network Quality-of-Service (QoS) and end-user QoE for video applications~\cite{balachandran2013developing,aggarwal2014prometheus,fiedler2010generic}, by using machine learning (ML) models and network features from PHY to TCP/UDP layers. ML models for QoE can be deployed at a number of vantage points in the network, such as access points, network controllers, cellular packet core. 

In order to train any QoS to QoE model, one needs accurate ground-truth annotation of the QoE. To obtain this, prior work has leveraged application specific APIs such as Skype technical information~\cite{zhang2012profiling} or YouTube API~\cite{aggarwal2014prometheus}; call duration~\cite{chen2006quantifying}; or instrumented client-side libraries for video delivery platform~\cite{balachandran2013developing}. While these solutions perform well for specific applications and providers, network administrators have to \emph{deal with a plethora of video applications} and they \emph{cannot control the application logic} for most applications. Nor does every application expose QoE metrics through APIs. 
Thus, to develop QoS to QoE models in the wild, network administrators need an application-independent approach to measure QoE. Note that application independence does not mean that modeling is application-agnostic, as a single QoE model cannot apply to all applications. Different applications exhibit diverse artefacts when quality deteriorates. For instance, streaming quality is impacted largely by buffering and stall ratio, whereas telephony quality is impacted by bit rate, frames per second, blocking and blurring in the video.

In this work, we propose a generic video telephony QoE model which does not rely on application support. Additionally, it is scalable to diverse content, devices and categories of video application. We take a similar approach as Jana {\em et al}~\cite{jana2016qoe}, where we record the screen on the mobile device and estimate video quality. Different from previous works, we exploit video compression methods (Section~\ref{label:background}) and identify four new metrics: {\em perceptual bitrate (PBR)}, {\em freeze ratio}, {\em length} and {\em number of video freezes} to measure 
the video QoE. We further demonstrate that our metrics are insensitive to the content of the video call and they only capture the {\em quality} of the video call (Section~\ref{label:results}). We make a rigorous analysis of our model performance by conducting a large scale user-study of 800 video clips across 20 videos and more than 200 users. We conduct an extensive analysis on different types of users' devices and OS (Android vs. iOS), video content and motion.  

We micro-benchmark our metrics under different network conditions to show that the metrics are agnostic to motion and content of the video.
Further, we show that these metrics capture spatial and temporal artefacts of video experience.
We then validate our metrics by mapping them to actual users' experience. To this end, we obtain Mean Opinion Score (MOS) from our user-study and apply ML models to map our metrics with users' MOS.
We use \texttt{Adaboosted} decision trees in predicting the MOS scores, as we describe in Section \ref{label:results}.


